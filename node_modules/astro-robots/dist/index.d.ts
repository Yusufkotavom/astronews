import { AstroIntegration } from 'astro';

type SearchEngines = {
    360: "360Spider" | "360Spider-Image" | "360Spider-Video" | "HaoSouSpider";
    Apple: "Applebot" | "AppleNewsBot";
    Baidu: "Baiduspider" | "Baiduspider-image" | "Baiduspider-mobile" | "Baiduspider-news" | "Baiduspider-video";
    Bing: "bingbot" | "BingPreview" | "msnbot" | "msnbot-media" | "adidxbot" | "MSN";
    Bublup: "BublupBot";
    Cliqz: "Cliqzbot";
    Coccoc: "coccoc" | "coccocbot-image" | "coccocbot-web";
    Daumoa: "Daumoa";
    Dazoo: "DeuSu";
    Duckduckgo: "DuckDuckBot" | "DuckDuckGo-Favicons-Bot";
    Eurip: "EuripBot";
    Exploratodo: "Exploratodo";
    Findx: "Findxbot";
    Goo: "gooblog" | "ichiro";
    Google: "Googlebot" | "Googlebot-Image" | "Googlebot-Mobile" | "Googlebot-News" | "Googlebot-Video" | "Mediapartners-Google" | "AdsBot-Google" | "AdsBot-Google-Mobile" | "AdsBot-Google-Mobile-Apps" | "Mediapartners-Google" | "Storebot-Google" | "Google-InspectionTool" | "FeedFetcher-Google";
    Istella: "istellabot";
    Jike: "JikeSpider";
    Lycos: "Lycos";
    Mail: "Mail.Ru";
    Mojeek: "MojeekBot";
    Orange: "OrangeBot";
    Botje: "Plukkie";
    Qwant: "Qwantify";
    Rambler: "Rambler";
    Seznam: "SeznamBot";
    Soso: "Sosospider";
    Yahoo: "Slurp";
    Sogou: "Sogou blog" | "Sogou inst spider" | "Sogou News Spider" | "Sogou Orion spider" | "Sogou spider2" | "Sogou web spider";
    Sputnik: "SputnikBot";
    Ask: "Teoma";
    Wortbox: "wotbox";
    Yandex: "Yandex" | "YandexMobileBot";
    Naver: "Yeti";
    Yioop: "YioopBot";
    Yooz: "yoozBot";
    Youdao: "YoudaoBot";
};
type SocialNetwork = {
    Facebook: "facebookcatalog" | "facebookexternalhit" | "Facebot";
    Pinterest: "Pinterest";
    Tittwer: "Twitterbot";
    WhatsApp: "WhatsApp";
    LinkedIn: "LinkedInBot";
};
type SearchEngineOptimization = {
    Ahrefs: "AhrefsBot";
    Moz: "Moz dotbot" | "Moz rogerbot";
    WebMeUp: "BLEXBot";
    Botify: "Botify";
    Babbar: "Barkrowler";
    SEMrush: "SEMrush" | "SemrushBotSI";
    Cxense: "Cxense";
    EzoicInc: "EzoicBot";
    DataForSEO: "DataForSEO";
    PrerenderLLC: "prerender";
};
type UserAgent = "*" | SearchEngines[keyof SearchEngines] | SocialNetwork[keyof SocialNetwork] | SearchEngineOptimization[keyof SearchEngineOptimization];

interface Policy {
    /**
     * @description
     * Indicates the robot to which the rules listed in "robots.txt" apply.
     * @example
     * ```ts
     * policy:[
     *  {
     *    userAgent: [
     *      'Googlebot',
     *      'Applebot',
     *      'Baiduspider',
     *      'bingbot'
     *    ],
     *    // crawling rule(s) for above bots
     *  }
     * ]
     * ```
     * Verified bots, refer to [DITIG](https://www.ditig.com/robots-txt-template#regular-template) or [Cloudflare Radar](https://radar.cloudflare.com/traffic/verified-bots).
     */
    userAgent: UserAgent | UserAgent[];
    /**
     * @description
     * [ At least one or more `allow` or `disallow` entries per rule ] Allows indexing site sections or individual pages.
     * @example
     * ```ts
     * policy:[{allow:["/"]}]
     * ```
     * Path-based URL matching, refer to [SYNTAX](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt#url-matching-based-on-path-values) via Google.
     */
    allow?: string | string[];
    /**
     * @description
     * [ At least one or more `disallow` or `allow` entries per rule ] Prohibits indexing site sections or individual pages.
     * @example
     * ```ts
     * policy:[
     *  {
     *    disallow:[
     *      "/admin",
     *      "/uploads/1989-08-21/*.jpg$"
     *    ]
     *  }
     * ]
     * ```
     * Path-based URL matching, refer to [SYNTAX](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt#url-matching-based-on-path-values) via Google.
     */
    disallow?: string | string[];
    /**
     * @description
     *Specifies the minimum interval (in seconds) for the search robot to wait after loading one page, before starting to load another.
     *
     * @example
     * ```ts
     * policy:[{crawlDelay:5}]
     * ```
     * About the [Crawl-delay](https://yandex.com/support/webmaster/robot-workings/crawl-delay.html#crawl-delay) directive.
     */
    crawlDelay?: number;
    /**
     * @description
     * Indicates to the robot that the page URL contains parameters (like UTM tags) that should be ignored when indexing it.
     *
     * @example
     * ```shell
     * # for URLs like:
     * www.example2.com/index.php?page=1&sid=2564126ebdec301c607e5df
     * www.example2.com/index.php?page=1&sid=974017dcd170d6c4a5d76ae
     * ```
     *
     * ```js
     * policy:[
     *  {
     *    cleanParam: [
     *      "sid /index.php",
     *    ]
     *  }
     * ]
     * ```
     * For more additional examples, see the
     * Yandex [SYNTAX](https://yandex.com/support/webmaster/robot-workings/clean-param.html#clean-param__additional) guide.
     */
    cleanParam?: string | string[];
}

interface RobotsOptions {
    /**
     * @description
     * Used to specify rules that apply to one or more robots.
     * @default
     * All robots are allowed.
     * ```ts
     * policy:[
     *  {
     *    userAgent: "*",
     *    allow: "/"
     *  }
     * ]
     * ```
     * For more help, refer to [SYNTAX](https://yandex.com/support/webmaster/controlling-robot/robots-txt.html#recommend) by Yandex.
     */
    policy: Policy[];
    /**
     * @description
     * The location of a sitemap for this website.
     * @example
     * ```ts
     * sitemap: [
     *  "https://example.com/sitemap.xml",
     *  "https://www.example.com/sitemap.xml"
     * ]
     * ```
     * The value of the [SITEMAP](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt#sitemap)
     * field is case-sensitive.
     */
    sitemap?: boolean | string | string[];
    /**
     * @default null
     * @description
     * Specify the value of `Host`, some crawlers(Yandex) support and only accept domain names.
     * @example
     * ```ts
     * host: siteUrl.replace(/^https?:\/\/|:\d+/g, "")
     * ```
     */
    host?: null | string;
}
declare function robots(options?: RobotsOptions): AstroIntegration;

export { type RobotsOptions, robots as default };
